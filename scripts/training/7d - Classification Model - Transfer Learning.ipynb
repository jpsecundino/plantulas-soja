{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating classification model\n",
    "##### Following this tutorial\n",
    "       www.youtube.com/watch?v=WvoLTXIjBYU\n",
    "##### Characteristics\n",
    " - Sequencial Model\n",
    " - Fully trained on this dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.3\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D\n",
    "import keras\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import cv2\n",
    "print(keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = '../../datasets/Classification/Dataset - 7 dias'\n",
    "CATEGORIES = [\"Normal\", \"Anormal\", \"Morta\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1184\n"
     ]
    }
   ],
   "source": [
    "images_raw = open(os.path.join(DATASET_PATH, \"7_days_images.pickle\" ), \"rb\")\n",
    "images = pickle.load(images_raw)\n",
    "\n",
    "labels_raw = open(os.path.join(DATASET_PATH, \"7_days_labels.pickle\" ), \"rb\")\n",
    "labels = pickle.load(labels_raw)\n",
    "\n",
    "#reshape images\n",
    "IMG_SIZE = 256\n",
    "\n",
    "print(len(images))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Train Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = np.array(images)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_images, test_images, train_labels, test_labels = train_test_split(images, labels, test_size=0.2, stratify=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In training:\n",
      "Class 0: 78.99%\n",
      "Class 1: 17.0%\n",
      "Class 2: 4.01%\n",
      "In test:\n",
      "Class 0: 78.9%\n",
      "Class 1: 16.88%\n",
      "Class 2: 4.22%\n"
     ]
    }
   ],
   "source": [
    "print(\"In training:\")\n",
    "for i in range(0,3):\n",
    "    print(f'Class {i}: {round(100 * np.count_nonzero(np.array(train_labels) == i) / len(train_labels), 2)}%')\n",
    "print(\"In test:\")\n",
    "for i in range(0,3):\n",
    "    print(f'Class {i}: {round(100 * np.count_nonzero(np.array(test_labels) == i) / len(test_labels), 2)}%')\n",
    "    \n",
    "# print(\"Proporção entre classes:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ajustando o dataset desbalanceado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Data augmentation -> data generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data augmentation with ImageDataGenerator -> did not go well\n",
    "# from keras.preprocessing.image import ImageDataGenerator\n",
    "# datagen = ImageDataGenerator(\n",
    "#     featurewise_center=True,\n",
    "#     featurewise_std_normalization=False,\n",
    "#     rotation_range=20,\n",
    "#     width_shift_range=0.2,\n",
    "#     height_shift_range=0.2,\n",
    "#     horizontal_flip=True,\n",
    "#     validation_split=0.25\n",
    "# )\n",
    "# datagen.fit(train_images)\n",
    "\n",
    "# # it = datagen.flow(train_images, batch_size=1, save_to_dir=\"test\", shuffle= True)\n",
    "\n",
    "# # # generate samples and plot\n",
    "# # for i in range(50):\n",
    "# #      # generate batch of images\n",
    "# #     batch = it.next()\n",
    "# #     # convert to unsigned integers for viewing\n",
    "# #     image = batch[0].astype('uint8')\n",
    "# #     # plot raw pixel data\n",
    "# #     plt.imshow(image, cmap=\"gray\")\n",
    "# #     plt.show()\n",
    "# # show the figure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Weighted Classes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Trying class weights\n",
    "#it does not work withone hot encoded data\n",
    "# from sklearn.utils import class_weight\n",
    "# class_weights = class_weight.compute_class_weight('balanced',\n",
    "#                                                  np.unique(train_labels),\n",
    "#                                                  train_labels)\n",
    "\n",
    "# from sklearn.utils import compute_sample_weight\n",
    "# sample_weights = compute_sample_weight('balanced', train_labels)  #->>>> remember to add the weights in model.fit\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classes, counts = np.unique(train_labels, return_counts=True)\n",
    "# print(f\"The number of elements for each class in training now are\\nClass 0: {counts[0]}\\nClass 1 {counts[1]}\\nClass 2 {counts[2]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from imblearn.over_sampling import SMOTE\n",
    "# sm = SMOTE(random_state=42)\n",
    "\n",
    "# train_images_shape = train_images.shape\n",
    "# train_images = train_images.reshape((train_images_shape[0], IMG_SIZE * IMG_SIZE * 3))\n",
    "# train_images, train_labels = sm.fit_resample(train_images, train_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_images_shape = train_images.shape\n",
    "# train_images = train_images.reshape((train_images_shape[0], IMG_SIZE, IMG_SIZE, 3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classes, counts = np.unique(train_labels, return_counts=True)\n",
    "# print(f\"The number of elements for each class in training now are\\nClass 0: {counts[0]}\\nClass 1 {counts[1]}\\nClass 2 {counts[2]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### One hot label encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "train_labels = pd.get_dummies(train_labels)\n",
    "train_labels = pd.DataFrame.to_numpy(train_labels)\n",
    "\n",
    "test_labels_1d = test_labels\n",
    "\n",
    "test_labels = pd.get_dummies(test_labels)\n",
    "test_labels = pd.DataFrame.to_numpy(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Normalizing training images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_images = tf.keras.utils.normalize(train_images, axis=0, order=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(256, 256, 3)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_images[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 15\n",
    "TRAIN_TEST_SPLIT = 0.25\n",
    "BATCH_SIZE = 32\n",
    "# print((1 - TRAIN_TEST_SPLIT) * train_images.shape[0],BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = keras.applications.Xception(\n",
    "    weights='imagenet',  # Load weights pre-trained on ImageNet.\n",
    "    input_shape=(IMG_SIZE, IMG_SIZE, 3),\n",
    "    include_top=False\n",
    ")\n",
    "\n",
    "base_model.trainable = False\n",
    "\n",
    "inputs = keras.Input(shape=(IMG_SIZE,IMG_SIZE, 3))\n",
    "# We make sure that the base_model is running in inference mode here,\n",
    "# by passing `training=False`. This is important for fine-tuning, as you will\n",
    "# learn in a few paragraphs.\n",
    "x = base_model(inputs)\n",
    "# Convert features of shape `base_model.output_shape[1:]` to vectors\n",
    "x = keras.layers.GlobalAveragePooling2D()(x)\n",
    "# A Dense classifier \n",
    "outputs = keras.layers.Dense(3, activation=\"sigmoid\")(x)\n",
    "\n",
    "model = keras.Model(inputs, outputs)\n",
    "\n",
    "my_metrics = [\"accuracy\",\n",
    "           tf.keras.metrics.Precision(),\n",
    "           tf.keras.metrics.Recall(),\n",
    "           tf.keras.metrics.AUC(),\n",
    "           tf.keras.metrics.TruePositives(),\n",
    "           tf.keras.metrics.TrueNegatives(),\n",
    "           tf.keras.metrics.FalsePositives(),\n",
    "           tf.keras.metrics.FalseNegatives(),]\n",
    "\n",
    "opt = keras.optimizers.Adam(learning_rate=0.00001)\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=opt, metrics=my_metrics)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.3\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "print(keras.__version__)\n",
    "METRICDIR = './metricas/classificacao/3_d/smote_transfer_learning/'\n",
    "MODELDIR = os.path.join(METRICDIR, \"model\")\n",
    "CHECKPOINT_DIR = os.path.join(MODELDIR, \"best_checkpoint\")\n",
    "checkpoint = ModelCheckpoint(CHECKPOINT_DIR, monitor=\"val_precision_1\", verbose=1,\n",
    "    save_best_only=True, mode='max', save_freq='epoch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "23/23 [==============================] - 84s 4s/step - loss: 1.7921 - accuracy: 0.3634 - precision_1: 0.3687 - recall_1: 0.7141 - auc_1: 0.5783 - true_positives_1: 507.0000 - true_negatives_1: 552.0000 - false_positives_1: 868.0000 - false_negatives_1: 203.0000 - val_loss: 1.7496 - val_accuracy: 0.3755 - val_precision_1: 0.3912 - val_recall_1: 0.7511 - val_auc_1: 0.6076 - val_true_positives_1: 178.0000 - val_true_negatives_1: 197.0000 - val_false_positives_1: 277.0000 - val_false_negatives_1: 59.0000\n",
      "\n",
      "Epoch 00001: val_precision_1 improved from -inf to 0.39121, saving model to ./metricas/classificacao/3_d/smote_transfer_learning/model/best_checkpoint\n",
      "INFO:tensorflow:Assets written to: ./metricas/classificacao/3_d/smote_transfer_learning/model/best_checkpoint/assets\n",
      "Epoch 2/15\n",
      "23/23 [==============================] - 103s 5s/step - loss: 1.5970 - accuracy: 0.4056 - precision_1: 0.3902 - recall_1: 0.7437 - auc_1: 0.6165 - true_positives_1: 528.0000 - true_negatives_1: 595.0000 - false_positives_1: 825.0000 - false_negatives_1: 182.0000 - val_loss: 1.5822 - val_accuracy: 0.4346 - val_precision_1: 0.4084 - val_recall_1: 0.7806 - val_auc_1: 0.6415 - val_true_positives_1: 185.0000 - val_true_negatives_1: 206.0000 - val_false_positives_1: 268.0000 - val_false_negatives_1: 52.0000\n",
      "\n",
      "Epoch 00002: val_precision_1 improved from 0.39121 to 0.40839, saving model to ./metricas/classificacao/3_d/smote_transfer_learning/model/best_checkpoint\n",
      "INFO:tensorflow:Assets written to: ./metricas/classificacao/3_d/smote_transfer_learning/model/best_checkpoint/assets\n",
      "Epoch 3/15\n",
      "23/23 [==============================] - 123s 5s/step - loss: 1.4293 - accuracy: 0.4380 - precision_1: 0.4152 - recall_1: 0.7859 - auc_1: 0.6516 - true_positives_1: 558.0000 - true_negatives_1: 634.0000 - false_positives_1: 786.0000 - false_negatives_1: 152.0000 - val_loss: 1.4437 - val_accuracy: 0.4684 - val_precision_1: 0.4244 - val_recall_1: 0.7932 - val_auc_1: 0.6699 - val_true_positives_1: 188.0000 - val_true_negatives_1: 219.0000 - val_false_positives_1: 255.0000 - val_false_negatives_1: 49.0000\n",
      "\n",
      "Epoch 00003: val_precision_1 improved from 0.40839 to 0.42438, saving model to ./metricas/classificacao/3_d/smote_transfer_learning/model/best_checkpoint\n",
      "INFO:tensorflow:Assets written to: ./metricas/classificacao/3_d/smote_transfer_learning/model/best_checkpoint/assets\n",
      "Epoch 4/15\n",
      "23/23 [==============================] - 124s 5s/step - loss: 1.2888 - accuracy: 0.4901 - precision_1: 0.4304 - recall_1: 0.8056 - auc_1: 0.6818 - true_positives_1: 572.0000 - true_negatives_1: 663.0000 - false_positives_1: 757.0000 - false_negatives_1: 138.0000 - val_loss: 1.3276 - val_accuracy: 0.4937 - val_precision_1: 0.4364 - val_recall_1: 0.8101 - val_auc_1: 0.6959 - val_true_positives_1: 192.0000 - val_true_negatives_1: 226.0000 - val_false_positives_1: 248.0000 - val_false_negatives_1: 45.0000\n",
      "\n",
      "Epoch 00004: val_precision_1 improved from 0.42438 to 0.43636, saving model to ./metricas/classificacao/3_d/smote_transfer_learning/model/best_checkpoint\n",
      "INFO:tensorflow:Assets written to: ./metricas/classificacao/3_d/smote_transfer_learning/model/best_checkpoint/assets\n",
      "Epoch 5/15\n",
      "23/23 [==============================] - 98s 4s/step - loss: 1.1783 - accuracy: 0.5352 - precision_1: 0.4465 - recall_1: 0.8338 - auc_1: 0.7091 - true_positives_1: 592.0000 - true_negatives_1: 686.0000 - false_positives_1: 734.0000 - false_negatives_1: 118.0000 - val_loss: 1.2285 - val_accuracy: 0.5401 - val_precision_1: 0.4541 - val_recall_1: 0.8354 - val_auc_1: 0.7168 - val_true_positives_1: 198.0000 - val_true_negatives_1: 236.0000 - val_false_positives_1: 238.0000 - val_false_negatives_1: 39.0000\n",
      "\n",
      "Epoch 00005: val_precision_1 improved from 0.43636 to 0.45413, saving model to ./metricas/classificacao/3_d/smote_transfer_learning/model/best_checkpoint\n",
      "INFO:tensorflow:Assets written to: ./metricas/classificacao/3_d/smote_transfer_learning/model/best_checkpoint/assets\n",
      "Epoch 6/15\n",
      "23/23 [==============================] - 111s 5s/step - loss: 1.0836 - accuracy: 0.5732 - precision_1: 0.4634 - recall_1: 0.8479 - auc_1: 0.7320 - true_positives_1: 602.0000 - true_negatives_1: 723.0000 - false_positives_1: 697.0000 - false_negatives_1: 108.0000 - val_loss: 1.1442 - val_accuracy: 0.5823 - val_precision_1: 0.4687 - val_recall_1: 0.8523 - val_auc_1: 0.7374 - val_true_positives_1: 202.0000 - val_true_negatives_1: 245.0000 - val_false_positives_1: 229.0000 - val_false_negatives_1: 35.0000\n",
      "\n",
      "Epoch 00006: val_precision_1 improved from 0.45413 to 0.46868, saving model to ./metricas/classificacao/3_d/smote_transfer_learning/model/best_checkpoint\n",
      "INFO:tensorflow:Assets written to: ./metricas/classificacao/3_d/smote_transfer_learning/model/best_checkpoint/assets\n",
      "Epoch 7/15\n",
      "23/23 [==============================] - 80s 4s/step - loss: 1.0041 - accuracy: 0.6197 - precision_1: 0.4804 - recall_1: 0.8648 - auc_1: 0.7527 - true_positives_1: 614.0000 - true_negatives_1: 756.0000 - false_positives_1: 664.0000 - false_negatives_1: 96.0000 - val_loss: 1.0720 - val_accuracy: 0.6160 - val_precision_1: 0.4835 - val_recall_1: 0.8650 - val_auc_1: 0.7554 - val_true_positives_1: 205.0000 - val_true_negatives_1: 255.0000 - val_false_positives_1: 219.0000 - val_false_negatives_1: 32.0000\n",
      "\n",
      "Epoch 00007: val_precision_1 improved from 0.46868 to 0.48349, saving model to ./metricas/classificacao/3_d/smote_transfer_learning/model/best_checkpoint\n",
      "INFO:tensorflow:Assets written to: ./metricas/classificacao/3_d/smote_transfer_learning/model/best_checkpoint/assets\n",
      "Epoch 8/15\n",
      "23/23 [==============================] - 78s 3s/step - loss: 0.9376 - accuracy: 0.6521 - precision_1: 0.4893 - recall_1: 0.8704 - auc_1: 0.7703 - true_positives_1: 618.0000 - true_negatives_1: 775.0000 - false_positives_1: 645.0000 - false_negatives_1: 92.0000 - val_loss: 1.0116 - val_accuracy: 0.6329 - val_precision_1: 0.4906 - val_recall_1: 0.8819 - val_auc_1: 0.7705 - val_true_positives_1: 209.0000 - val_true_negatives_1: 257.0000 - val_false_positives_1: 217.0000 - val_false_negatives_1: 28.0000\n",
      "\n",
      "Epoch 00008: val_precision_1 improved from 0.48349 to 0.49061, saving model to ./metricas/classificacao/3_d/smote_transfer_learning/model/best_checkpoint\n",
      "INFO:tensorflow:Assets written to: ./metricas/classificacao/3_d/smote_transfer_learning/model/best_checkpoint/assets\n",
      "Epoch 9/15\n",
      "23/23 [==============================] - 77s 3s/step - loss: 0.8817 - accuracy: 0.6732 - precision_1: 0.4980 - recall_1: 0.8831 - auc_1: 0.7859 - true_positives_1: 627.0000 - true_negatives_1: 788.0000 - false_positives_1: 632.0000 - false_negatives_1: 83.0000 - val_loss: 0.9543 - val_accuracy: 0.6498 - val_precision_1: 0.5048 - val_recall_1: 0.8945 - val_auc_1: 0.7852 - val_true_positives_1: 212.0000 - val_true_negatives_1: 266.0000 - val_false_positives_1: 208.0000 - val_false_negatives_1: 25.0000\n",
      "\n",
      "Epoch 00009: val_precision_1 improved from 0.49061 to 0.50476, saving model to ./metricas/classificacao/3_d/smote_transfer_learning/model/best_checkpoint\n",
      "INFO:tensorflow:Assets written to: ./metricas/classificacao/3_d/smote_transfer_learning/model/best_checkpoint/assets\n",
      "Epoch 10/15\n",
      "23/23 [==============================] - 64s 3s/step - loss: 0.8307 - accuracy: 0.6986 - precision_1: 0.5060 - recall_1: 0.8845 - auc_1: 0.7986 - true_positives_1: 628.0000 - true_negatives_1: 807.0000 - false_positives_1: 613.0000 - false_negatives_1: 82.0000 - val_loss: 0.9040 - val_accuracy: 0.6667 - val_precision_1: 0.5144 - val_recall_1: 0.9030 - val_auc_1: 0.7961 - val_true_positives_1: 214.0000 - val_true_negatives_1: 272.0000 - val_false_positives_1: 202.0000 - val_false_negatives_1: 23.0000\n",
      "\n",
      "Epoch 00010: val_precision_1 improved from 0.50476 to 0.51442, saving model to ./metricas/classificacao/3_d/smote_transfer_learning/model/best_checkpoint\n",
      "INFO:tensorflow:Assets written to: ./metricas/classificacao/3_d/smote_transfer_learning/model/best_checkpoint/assets\n",
      "Epoch 11/15\n",
      "23/23 [==============================] - 60s 3s/step - loss: 0.7833 - accuracy: 0.7338 - precision_1: 0.5175 - recall_1: 0.8972 - auc_1: 0.8108 - true_positives_1: 637.0000 - true_negatives_1: 826.0000 - false_positives_1: 594.0000 - false_negatives_1: 73.0000 - val_loss: 0.8587 - val_accuracy: 0.7004 - val_precision_1: 0.5216 - val_recall_1: 0.9156 - val_auc_1: 0.8077 - val_true_positives_1: 217.0000 - val_true_negatives_1: 275.0000 - val_false_positives_1: 199.0000 - val_false_negatives_1: 20.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00011: val_precision_1 improved from 0.51442 to 0.52163, saving model to ./metricas/classificacao/3_d/smote_transfer_learning/model/best_checkpoint\n",
      "INFO:tensorflow:Assets written to: ./metricas/classificacao/3_d/smote_transfer_learning/model/best_checkpoint/assets\n",
      "Epoch 12/15\n",
      "23/23 [==============================] - 73s 3s/step - loss: 0.7413 - accuracy: 0.7507 - precision_1: 0.5301 - recall_1: 0.9056 - auc_1: 0.8210 - true_positives_1: 643.0000 - true_negatives_1: 850.0000 - false_positives_1: 570.0000 - false_negatives_1: 67.0000 - val_loss: 0.8175 - val_accuracy: 0.7131 - val_precision_1: 0.5301 - val_recall_1: 0.9283 - val_auc_1: 0.8172 - val_true_positives_1: 220.0000 - val_true_negatives_1: 279.0000 - val_false_positives_1: 195.0000 - val_false_negatives_1: 17.0000\n",
      "\n",
      "Epoch 00012: val_precision_1 improved from 0.52163 to 0.53012, saving model to ./metricas/classificacao/3_d/smote_transfer_learning/model/best_checkpoint\n",
      "INFO:tensorflow:Assets written to: ./metricas/classificacao/3_d/smote_transfer_learning/model/best_checkpoint/assets\n",
      "Epoch 13/15\n",
      "23/23 [==============================] - 66s 3s/step - loss: 0.7032 - accuracy: 0.7606 - precision_1: 0.5414 - recall_1: 0.9127 - auc_1: 0.8301 - true_positives_1: 648.0000 - true_negatives_1: 871.0000 - false_positives_1: 549.0000 - false_negatives_1: 62.0000 - val_loss: 0.7843 - val_accuracy: 0.7215 - val_precision_1: 0.5353 - val_recall_1: 0.9283 - val_auc_1: 0.8255 - val_true_positives_1: 220.0000 - val_true_negatives_1: 283.0000 - val_false_positives_1: 191.0000 - val_false_negatives_1: 17.0000\n",
      "\n",
      "Epoch 00013: val_precision_1 improved from 0.53012 to 0.53528, saving model to ./metricas/classificacao/3_d/smote_transfer_learning/model/best_checkpoint\n",
      "INFO:tensorflow:Assets written to: ./metricas/classificacao/3_d/smote_transfer_learning/model/best_checkpoint/assets\n",
      "Epoch 14/15\n",
      "23/23 [==============================] - 62s 3s/step - loss: 0.6690 - accuracy: 0.7732 - precision_1: 0.5478 - recall_1: 0.9197 - auc_1: 0.8377 - true_positives_1: 653.0000 - true_negatives_1: 881.0000 - false_positives_1: 539.0000 - false_negatives_1: 57.0000 - val_loss: 0.7542 - val_accuracy: 0.7257 - val_precision_1: 0.5443 - val_recall_1: 0.9325 - val_auc_1: 0.8332 - val_true_positives_1: 221.0000 - val_true_negatives_1: 289.0000 - val_false_positives_1: 185.0000 - val_false_negatives_1: 16.0000\n",
      "\n",
      "Epoch 00014: val_precision_1 improved from 0.53528 to 0.54433, saving model to ./metricas/classificacao/3_d/smote_transfer_learning/model/best_checkpoint\n",
      "INFO:tensorflow:Assets written to: ./metricas/classificacao/3_d/smote_transfer_learning/model/best_checkpoint/assets\n",
      "Epoch 15/15\n",
      "23/23 [==============================] - 76s 3s/step - loss: 0.6381 - accuracy: 0.7803 - precision_1: 0.5581 - recall_1: 0.9268 - auc_1: 0.8449 - true_positives_1: 658.0000 - true_negatives_1: 899.0000 - false_positives_1: 521.0000 - false_negatives_1: 52.0000 - val_loss: 0.7271 - val_accuracy: 0.7511 - val_precision_1: 0.5522 - val_recall_1: 0.9367 - val_auc_1: 0.8406 - val_true_positives_1: 222.0000 - val_true_negatives_1: 294.0000 - val_false_positives_1: 180.0000 - val_false_negatives_1: 15.0000\n",
      "\n",
      "Epoch 00015: val_precision_1 improved from 0.54433 to 0.55224, saving model to ./metricas/classificacao/3_d/smote_transfer_learning/model/best_checkpoint\n",
      "INFO:tensorflow:Assets written to: ./metricas/classificacao/3_d/smote_transfer_learning/model/best_checkpoint/assets\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_images, train_labels, batch_size = BATCH_SIZE, epochs = EPOCHS, validation_split=TRAIN_TEST_SPLIT, callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Loading the best model for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model(CHECKPOINT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 1, 0, 2, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 2, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 2, 1, 0, 0, 0, 2, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 2, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[[1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [0 1 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [0 0 1]\n",
      " [0 1 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [0 1 0]\n",
      " [1 0 0]\n",
      " [0 0 1]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [0 1 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [0 1 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [0 0 1]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [0 1 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [0 1 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [0 1 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [0 1 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [0 1 0]\n",
      " [0 1 0]\n",
      " [0 1 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [0 1 0]\n",
      " [1 0 0]\n",
      " [0 1 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [0 1 0]\n",
      " [0 1 0]\n",
      " [0 0 1]\n",
      " [1 0 0]\n",
      " [0 1 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [0 1 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [0 1 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [0 1 0]\n",
      " [0 1 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [0 1 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [0 1 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [0 1 0]\n",
      " [1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]\n",
      " [0 1 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [0 0 1]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [0 1 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [0 1 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [0 1 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [0 1 0]\n",
      " [1 0 0]\n",
      " [0 1 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [0 0 1]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [0 0 1]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [0 0 1]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [0 1 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [0 1 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [0 1 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [0 0 1]\n",
      " [0 1 0]\n",
      " [0 1 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [0 1 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [0 1 0]\n",
      " [1 0 0]\n",
      " [0 1 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [0 1 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]]\n"
     ]
    }
   ],
   "source": [
    "test_labels_1d = []\n",
    "for label in test_labels:\n",
    "    if (label == [1, 0, 0]).all():\n",
    "        test_labels_1d.append(0)\n",
    "    elif (label == [0, 1, 0]).all():\n",
    "        test_labels_1d.append(1)\n",
    "    else:\n",
    "        test_labels_1d.append(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model.predict(test_images, batch_size=1)\n",
    "\n",
    "prediction = tf.argmax(prediction, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.927     0.813     0.866       187\n",
      "           1      0.500     0.625     0.556        40\n",
      "           2      0.391     0.900     0.545        10\n",
      "\n",
      "    accuracy                          0.785       237\n",
      "   macro avg      0.606     0.779     0.656       237\n",
      "weighted avg      0.832     0.785     0.800       237\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "print(metrics.classification_report(test_labels_1d, prediction, digits=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cm = metrics.confusion_matrix(test_labels_1d, prediction)\n",
    "\n",
    "df_cm = pd.DataFrame(cm, ['Normal', 'Anormal', 'Morta'], ['Normal', 'Anormal', 'Morta'])\n",
    "plt.figure(figsize=(5,5))\n",
    "sn.set(font_scale=1.2) # for label size\n",
    "sn.heatmap(df_cm, annot=True, annot_kws={\"size\": 10}) # font size\n",
    "plt.yticks(np.arange(3) + 0.8,('Normal','Anormal','Morta'), rotation=0, fontsize=\"10\", va=\"center\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualizing model data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_METRICS = len(my_metrics) + 1\n",
    "\n",
    "SAVE = True\n",
    "\n",
    "def plot_metric(metric_name, chart_name):\n",
    "    plt.plot(history.history[metric_name], label = metric_name + ' (training data)')\n",
    "    plt.plot(history.history['val_' + metric_name], label = metric_name + ' (validation data)')\n",
    "    plt.title(chart_name)\n",
    "    plt.ylabel( metric_name + ' value')\n",
    "    plt.xlabel('No. epoch')\n",
    "    plt.legend(loc=\"upper left\")\n",
    "    if SAVE and not os.path.exists(METRICDIR):\n",
    "        os.mkdir(METRICDIR)\n",
    "    if SAVE:\n",
    "        plt.savefig(os.path.join(METRICDIR, metric_name))\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for metric in list(history.history.keys())[:NUM_METRICS]:\n",
    "    plot_metric(metric, metric + \" for soybean classification\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
